{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d2ca369",
   "metadata": {},
   "source": [
    "First we start with our classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aa71e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from random import random\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dbf86d",
   "metadata": {},
   "source": [
    "we imported the relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a8b8095",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, score=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.score = score\n",
    "        self.value = value\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier():\n",
    "    def __init__(self, min_samples_split=5, max_depth=3, X_idxs=[], Y_idx=[],cols=[]):\n",
    "        self.root = None\n",
    "        self.cols=cols\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.X_idxs = X_idxs\n",
    "        self.Y_idx = Y_idx\n",
    "\n",
    "    def build_tree(self, dataset, curr_depth=0):\n",
    "        num_samples = dataset.shape[0]\n",
    "        if num_samples >= self.min_samples_split and curr_depth < self.max_depth:\n",
    "            num_B = dataset.loc[dataset.area_type == 1].shape[0]\n",
    "            num_P = num_samples-num_B\n",
    "            best_split = self.find_best_split(dataset, num_B, num_P)\n",
    "            if best_split[\"score\"] < self.get_Gini(num_B, num_P):\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth + 1)\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth + 1)\n",
    "                return Node(best_split[\"feature\"], best_split[\"threshold\"],\n",
    "                            left_subtree, right_subtree, best_split[\"score\"])\n",
    "        leaf_value = self.leaf_value(dataset)\n",
    "        return Node(value=leaf_value)\n",
    "\n",
    "    def find_best_split(self, dataset, num_B, num_P):\n",
    "        nump=dataset.to_numpy()\n",
    "        best_split = {}\n",
    "        max_score = float(\"inf\")\n",
    "        for feature in self.X_idxs:\n",
    "            sorted_data = nump[nump[:, feature].argsort()]\n",
    "            group1_B_count = 0\n",
    "            group1_P_count = 0\n",
    "            group2_B_count = num_B\n",
    "            group2_P_count = num_P\n",
    "            prev = -1\n",
    "            for i in range(0,len(sorted_data)):\n",
    "                if prev != -1:\n",
    "                    if sorted_data[prev][feature] != sorted_data[i][feature]:\n",
    "                        score = self.get_score(group1_B_count, group1_P_count, group2_B_count, group2_P_count)\n",
    "                        if score <= max_score:\n",
    "                            best_split[\"feature\"] = feature\n",
    "                            best_split[\"score\"] = score\n",
    "                            max_score = score\n",
    "                            if prev != -1:\n",
    "                                best_split[\"threshold\"] = (sorted_data[i][feature] + sorted_data[prev][feature]) / 2\n",
    "                            else:\n",
    "                                best_split[\"threshold\"] = sorted_data[i][feature]\n",
    "                if sorted_data[i][self.Y_idx] == 1:\n",
    "                    group1_B_count += 1\n",
    "                    group2_B_count -= 1\n",
    "                else:\n",
    "                    group1_P_count += 1\n",
    "                    group2_P_count -= 1\n",
    "                prev = i\n",
    "        best_split[\"dataset_left\"], best_split[\"dataset_right\"] = self.split(dataset, best_split[\"feature\"],\n",
    "                                                                             best_split[\"threshold\"])\n",
    "        return best_split\n",
    "\n",
    "    def split(self, dataset, feature, threshold):\n",
    "        dataset_right = dataset.loc[dataset[self.cols[feature]] >= threshold]\n",
    "        dataset_left = dataset.loc[dataset[self.cols[feature]] < threshold]\n",
    "        return dataset_left, dataset_right\n",
    "\n",
    "    def get_Gini(self, ycount_1, ycount_2) -> float:\n",
    "        n = ycount_1 + ycount_2\n",
    "        if n == 0:\n",
    "            return 0.0\n",
    "        p1 = ycount_1 / n\n",
    "        p2 = ycount_2 / n\n",
    "        return 1.0 - (p1 ** 2 + p2 ** 2)\n",
    "\n",
    "    def get_score(self, group1_B_count, group1_P_count, group2_B_count, group2_P_count) -> float:\n",
    "        gini1 = self.get_Gini(group1_B_count, group1_P_count)\n",
    "        gini2 = self.get_Gini(group2_B_count, group2_P_count)\n",
    "        group1_count = group1_B_count + group1_P_count\n",
    "        group2_count = group2_B_count + group2_P_count\n",
    "        n = group1_count + group2_count\n",
    "        gini = gini1 * (group1_count / n) + gini2 * (group2_count / n)\n",
    "        return gini\n",
    "\n",
    "    def leaf_value(self, dataset):\n",
    "        values = list(dataset.iloc[:,self.Y_idx].values)\n",
    "        return max(values, key=values.count)\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        dataset=dataset.to_numpy()\n",
    "        length=len(dataset)\n",
    "        y = [self.predict_sample(index, self.root, dataset) for index in range(0, length)]\n",
    "        return y\n",
    "\n",
    "    def predict_sample(self, index, tree, dataset):\n",
    "        if tree.value != None:\n",
    "            return tree.value\n",
    "        feature = dataset[index][tree.feature]\n",
    "        if feature < tree.threshold:\n",
    "            return self.predict_sample(index, tree.left, dataset)\n",
    "        else:\n",
    "            return self.predict_sample(index, tree.right, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeef098",
   "metadata": {},
   "source": [
    "above is the class of the Decision Tree, we start by defining a new class \"Node\" which is one node in the DT. after we defined the \"DecisionTreeClassifier\" class which is going to be our tree. then we defined a method that's responsible of building the tree \"build_tree\". after that there's a method that's responsible of finding the best split in our dataset using the B or P values \"find_best_split\", in it the loop goes over our dataset after it being sorted and appended into a numpy array, and checks the score for the split and checks if the score we got is better than the current score, we change it else we don't. the \"split\" method is used to actually split the dataset. then we calculate the gini using the \"get_gini\" method the \"get_score\" method is used to determine the gini of the current split.\n",
    "\n",
    "at the end is our predict methods.\n",
    "\n",
    "now we start with the Adaboost classifier class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d976eeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stump:\n",
    "    def __init__(self, buildset=None, X_idxs=[], Y_idx=[],cols=[], weight=None,original_nump=None, tree=None):\n",
    "        self.original_nump=original_nump\n",
    "        self.weight=weight\n",
    "        self.X_idxs = X_idxs\n",
    "        self.Y_idx = Y_idx\n",
    "        self.cols=cols\n",
    "        self.tree = DecisionTreeClassifier( max_depth=1, X_idxs=X_idxs, Y_idx=Y_idx,cols=self.cols)\n",
    "        self.tree.root = self.tree.build_tree(buildset)\n",
    "        self.buildset = buildset\n",
    "        err = self.calc_err()\n",
    "        self.weight = 1 / 2 * math.log((1 - err) / err)\n",
    "\n",
    "    def calc_err(self):\n",
    "        errors = 0\n",
    "        for index in range(0,len(self.original_nump)):\n",
    "            pred = self.tree.predict_sample(index, self.tree.root, self.original_nump)\n",
    "            if pred != self.original_nump[index][Y_index]:\n",
    "                errors += self.weight[index]\n",
    "        return errors\n",
    "\n",
    "\n",
    "class AdaBoost:\n",
    "    def __init__(self, stumps_num=None, stumps=[], X_idxs=[], Y_idx=[], cols=[]):\n",
    "        self.cols=cols\n",
    "        self.stumps = stumps\n",
    "        self.stumps_num=stumps_num\n",
    "        self.X_idxs = X_idxs\n",
    "        self.Y_idx = Y_idx\n",
    "\n",
    "    def buildAdaBoost(self, dataset,weight=[]):\n",
    "        length = dataset.shape[0]\n",
    "        w = 1 / length\n",
    "        if(len(weight)==0):\n",
    "            weight = np.full(length, w)\n",
    "        nump = dataset.to_numpy()\n",
    "        while(len(self.stumps)<self.stumps_num):\n",
    "            wc = np.zeros(length)\n",
    "            st = Stump(dataset, self.X_idxs, self.Y_idx,cols=self.cols,weight=weight,original_nump=nump)\n",
    "            self.stumps.append(st)\n",
    "            for index in range(0,length):\n",
    "                pred = st.tree.predict_sample(index, st.tree.root, nump)\n",
    "                if pred != nump[index][self.Y_idx]:\n",
    "                    weight[index] *= (math.e ** (st.weight))\n",
    "                else:\n",
    "                    weight[index] *= (math.e ** (-st.weight))\n",
    "            sum_w = sum(weight)\n",
    "            weight /= sum_w\n",
    "            sum_w = 0\n",
    "            for index in range(0, length):\n",
    "                sum_w += weight[index]\n",
    "                wc[index] = sum_w\n",
    "            dataset = self.make_new_dataset(nump, wc)\n",
    "        return weight\n",
    "    def make_new_dataset(self, nump, wc):\n",
    "        sorted_rand = np.zeros(nump.shape[0])\n",
    "        for i in range(0, nump.shape[0]):\n",
    "            x = random()\n",
    "            sorted_rand[i] = x\n",
    "        sorted_rand.sort()\n",
    "        new_data = np.zeros(nump.shape[0])\n",
    "        i,j=0,0\n",
    "        while (j < nump.shape[0]):\n",
    "            if (wc[i] > sorted_rand[j]):\n",
    "                new_data[j] = i\n",
    "                j += 1\n",
    "            else:\n",
    "                i += 1\n",
    "        new_data = np.int_(new_data)\n",
    "        new_data_np = nump.copy()\n",
    "        i = 0\n",
    "        for index in new_data:\n",
    "            for j in range(0, nump.shape[1]):\n",
    "                new_data_np[i][j] = nump[index][j]\n",
    "            i += 1\n",
    "        return pd.DataFrame(new_data_np, columns=col_names)\n",
    "    def predict(self, X):\n",
    "        y = [self.predict_sample(index, X) for index in range(0, len(X))]\n",
    "        return y\n",
    "\n",
    "    def predict_sample(self, index, dataset):\n",
    "        weight_b = 0\n",
    "        weight_p = 0\n",
    "        dataset=dataset.to_numpy()\n",
    "        for st in self.stumps:\n",
    "            res = st.tree.predict_sample(index, st.tree.root, dataset)\n",
    "            if res == 1:\n",
    "                weight_b += st.weight\n",
    "            if res == 0:\n",
    "                weight_p += st.weight\n",
    "        if weight_b > weight_p:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b27385",
   "metadata": {},
   "source": [
    "we start by defining a new class \"Stump\" that replaces the \"Node\" in the decision tree, since in Adaboost each node is a stump which is a one-level-decision tree.\n",
    "\n",
    "after that we have the \"calc_err\" that's used to calculate the errors we get in each run.\n",
    "\n",
    "then we defined the adaboost class \"AdaBoost\". and in the end \"Make_new_dataset\" is responsible of adjusting our dataset according to the errors we got in the last stump.\n",
    "\n",
    "at the end is the predict of the Adaboost class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1207fb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing 2 functions to calculate the sensitivity and specifity for the bonus section:\n",
    "def sensitivity(Y,Y_pred,positive,negative):\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    for y,y_pred in zip(Y,Y_pred):\n",
    "        if(y==y_pred and y==positive):\n",
    "             TP+=1\n",
    "        if (y != y_pred and y_pred == negative):\n",
    "             FN += 1\n",
    "    return TP/(FN+TP)\n",
    "def specificity(Y,Y_pred,positive,negative):\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    for y,y_pred in zip(Y,Y_pred):\n",
    "        if(y==y_pred and y==negative):\n",
    "             TN+=1\n",
    "        if (y != y_pred and y_pred == positive):\n",
    "             FP += 1\n",
    "    return TN/(FP+TN)\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20034dd5",
   "metadata": {},
   "source": [
    "define X and y and converting the data into numbers and spliting the data into training, validation, testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bb3083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_index = 0\n",
    "X_indexes = [i for i in range(1,8)]\n",
    "col_names = ['area_type', 'availability', 'bedrooms', 'total_sqft', 'bath', 'balcony', 'ranked', 'price in rupees']\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"data.csv\", skiprows=1, header=None, names=col_names)\n",
    "map = {'B': 1, 'P': 0}\n",
    "data.area_type = [map[item] for item in data.area_type]\n",
    "training_data = data.iloc[:8041, :]\n",
    "train_X=training_data.iloc[:,1:]\n",
    "train_y=training_data.iloc[:,0]\n",
    "validation_data = data.iloc[8041:10051, :]\n",
    "valid_X=validation_data.iloc[:,1:]\n",
    "valid_y=validation_data.iloc[:,0]\n",
    "testing_data = data.iloc[10051:, :]\n",
    "test_X=testing_data.iloc[:,1:]\n",
    "test_y=testing_data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874104ed",
   "metadata": {},
   "source": [
    "now we build 10 decision trees using the hyper parameter \"max-depth\", then using validation data to determine which depth is best, we are checking max-depths 2-12:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "711efb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of 2 max depth is 0.8950248756218906\n",
      "accuracy of 3 max depth is 0.8955223880597015\n",
      "accuracy of 4 max depth is 0.9014925373134328\n",
      "accuracy of 5 max depth is 0.9039800995024876\n",
      "accuracy of 6 max depth is 0.9099502487562189\n",
      "accuracy of 7 max depth is 0.9109452736318407\n",
      "accuracy of 8 max depth is 0.908955223880597\n",
      "accuracy of 9 max depth is 0.9114427860696518\n",
      "accuracy of 10 max depth is 0.9114427860696518\n",
      "accuracy of 11 max depth is 0.9109452736318407\n",
      "accuracy of 12 max depth is 0.908955223880597\n",
      "best validation accuracy is using 9 max depth with accuracy of 0.9114427860696518\n",
      "Time it took for the tree to get built:10.638726711273193 seconds\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "max_acc=0\n",
    "best_classifier=None\n",
    "for i in range(2,13):\n",
    "    classifier = DecisionTreeClassifier(max_depth=i, X_idxs=X_indexes, Y_idx=Y_index,\n",
    "                                        cols=col_names)\n",
    "    classifier.root = classifier.build_tree(training_data)\n",
    "    Y_pred = classifier.predict(validation_data)\n",
    "    acc=accuracy_score(valid_y, Y_pred)\n",
    "    print(\"accuracy of \"+str(i)+\" max depth is \"+str(acc))\n",
    "    if(acc>max_acc):\n",
    "        max_acc=acc\n",
    "        best_classifier=classifier\n",
    "        best_sts_num=i\n",
    "end = time.time()\n",
    "print(\"best validation accuracy is using \"+str(best_sts_num)+\" max depth with accuracy of \"+ str(max_acc))\n",
    "print(\"Time it took for the tree to get built:\"+str(end-start)+\" seconds\")\n",
    "print(\"****************************************************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7044638",
   "metadata": {},
   "source": [
    "in our code, we used the \"max_depth\" as our hyper parameter in decision tree classifier. from the prints we can see that the program found that at the max_depth of 9 we have the best accuracy of 0.9114427860696518."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0585c2",
   "metadata": {},
   "source": [
    "testing our decision tree best classifier with the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c05b9bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy with the best classifier\n",
      "0.910031847133758\n"
     ]
    }
   ],
   "source": [
    "Y_pred = best_classifier.predict(testing_data)\n",
    "print(\"testing accuracy with the best classifier\")\n",
    "print(accuracy_score(test_y, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7957d856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity is: 0.9505087881591119 and the Specificity is: 0.66\n"
     ]
    }
   ],
   "source": [
    "print(\"Sensitivity is: \" +str(sensitivity(test_y,Y_pred,positive=1,negative=0))+\n",
    "      \" and the Specificity is: \"+str(specificity(test_y,Y_pred,positive=1,negative=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c9a9a2",
   "metadata": {},
   "source": [
    "our model is much more accurate when predicting B type area, since our sensitivity is very high. yet it seems that the model is not as good as predicting P type area. A test with low specificity can be thought of as being too eager to find a positive result, even when it is not present, and may give a high number of false positives, which in our case - is classifying P as B. we're speculating that our dataset has a lot of rows that have \"Area_type = B\" in comparession to \"Area_type = P\" which is causing the classifier to be much more sensitive when identifying a \"B\" - resulting in high sensitivity when we set B as our positive value and a high specificity value when setting B as our negative value. our solution would be adding more data to our dataset that contain \"Area_type = P\" ,this will help the algorithm to learn more on P values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bdb54f",
   "metadata": {},
   "source": [
    "\n",
    "implementing the <b>SKLearn</b> code to decision tree classifier:</br>\n",
    "<font color='red'>NOTE: We didn't use gridsreach, we felt more comfortable using our own loop, one of the reasons was because we couldn't find online a way to use predefined validation data into gridsearch</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dff00dfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of 2 max depth is 0.8950248756218906\n",
      "accuracy of 3 max depth is 0.8955223880597015\n",
      "accuracy of 4 max depth is 0.9009950248756219\n",
      "accuracy of 5 max depth is 0.9039800995024876\n",
      "accuracy of 6 max depth is 0.9109452736318407\n",
      "accuracy of 7 max depth is 0.9109452736318407\n",
      "accuracy of 8 max depth is 0.9099502487562189\n",
      "accuracy of 9 max depth is 0.9074626865671642\n",
      "accuracy of 10 max depth is 0.9054726368159204\n",
      "accuracy of 11 max depth is 0.9084577114427861\n",
      "accuracy of 12 max depth is 0.9064676616915422\n",
      "Decision tree classifier best validation accuracy is using 6 max depth with accuracy of 0.9109452736318407\n",
      "testing accuracy with the best classifier is:0.910828025477707\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_acc=0\n",
    "best_classifier=None\n",
    "for i in range(2,13):\n",
    "    classifier = tree.DecisionTreeClassifier(min_samples_split=5,max_depth=i,random_state=10)\n",
    "    classifier = classifier.fit(train_X,train_y)\n",
    "    Y_pred = classifier.predict(valid_X)\n",
    "    acc=accuracy_score(valid_y, Y_pred)\n",
    "    print(\"accuracy of \"+str(i)+\" max depth is \"+str(acc))\n",
    "    if(acc>max_acc):\n",
    "        max_acc=acc\n",
    "        best_classifier=classifier\n",
    "        best_sts_num=i\n",
    "print(\"Decision tree classifier best validation accuracy is using \"+str(best_sts_num)+\" max depth with accuracy of \"+ str(max_acc))\n",
    "Y_pred = best_classifier.predict(test_X)\n",
    "print(\"testing accuracy with the best classifier is:\"+ str(accuracy_score(test_y, Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7ceac6",
   "metadata": {},
   "source": [
    "We got similar results when testing our model's accuracy vs the sklearn model's accuracy when it came to the validation data, since we can see that testing the validation using our model resulted in an accuracy of 0.9114427860696518 meanwhile using sklearn we got 0.9109452736318407. meanwhile when we tested our accuracy using the \"testing\" data, we got a score of 0.910031847133758, in the sklearn model 0.910828025477707."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6653a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of 10 stumps is 0.8771144278606965\n",
      "accuracy of 20 stumps is 0.8835820895522388\n",
      "accuracy of 30 stumps is 0.8865671641791045\n",
      "accuracy of 40 stumps is 0.8880597014925373\n",
      "accuracy of 50 stumps is 0.8875621890547264\n",
      "accuracy of 60 stumps is 0.8940298507462686\n",
      "accuracy of 70 stumps is 0.8940298507462686\n",
      "accuracy of 80 stumps is 0.8920398009950249\n",
      "accuracy of 90 stumps is 0.8950248756218906\n",
      "accuracy of 100 stumps is 0.8935323383084577\n",
      "best validation accuracy is using 90 stumps with accuracy of 0.8950248756218906\n",
      "Time it took for the tree to get built:122.50025343894958 seconds\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "from sklearn.metrics import accuracy_score\n",
    "max_acc=0\n",
    "best_classifier=None\n",
    "stumps=[]\n",
    "weight=[]\n",
    "for i in range(10,110,10):\n",
    "    classifier = AdaBoost(i, stumps, X_idxs=X_indexes, Y_idx=Y_index,cols=col_names)\n",
    "    weight=classifier.buildAdaBoost(training_data,weight=weight)\n",
    "    stumps=classifier.stumps\n",
    "    Y_pred = classifier.predict(validation_data)\n",
    "    acc=accuracy_score(valid_y, Y_pred)\n",
    "    print(\"accuracy of \"+str(i)+\" stumps is \"+str(acc))\n",
    "    if(acc>max_acc):\n",
    "        max_acc=acc\n",
    "        best_classifier=classifier\n",
    "        best_sts_num=i\n",
    "end = time.time()\n",
    "print(\"best validation accuracy is using \"+str(best_sts_num)+\" stumps with accuracy of \"+ str(max_acc))\n",
    "print(\"Time it took for the tree to get built:\"+str(end-start)+\" seconds\")\n",
    "print(\"****************************************************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e23b89",
   "metadata": {},
   "source": [
    "in the Adaboost class we used number of stumps and checked the accuarcy every 10 stumps. we start off by creating 10 stumps and check the accuracy, then we create an additional 10 stumps on our previous adaboost model, and check the accuracy again, and so on. we can see from the prints the we got that the best validation accuracy is using 90 stumps with accuracy of 0.8950248756218906"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aff7b632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy with the best classifier\n",
      "0.8929140127388535\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Y_pred = best_classifier.predict(testing_data)\n",
    "print(\"testing accuracy with the best classifier\")\n",
    "print(accuracy_score(test_y, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0945a1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity is: 0.9592969472710453 and the Specificity is: 0.4828571428571429\n"
     ]
    }
   ],
   "source": [
    "print(\"Sensitivity is: \" +str(sensitivity(test_y,Y_pred,positive=1,negative=0))+\n",
    "      \" and the Specificity is: \"+str(specificity(test_y,Y_pred,positive=1,negative=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d67f50",
   "metadata": {},
   "source": [
    "in comparison to the decision tree, the sensitivity is higher and the specificity is lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0e84ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost classifier best validation accuracy is using 70 max depth with accuracy of 0.9099502487562189\n",
      "testing accuracy with the best classifier is:0.902468152866242\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingRegressor\n",
    "max_acc=0\n",
    "best_classifier=None\n",
    "for i in range(10,110,10):\n",
    "    classifier = AdaBoostClassifier(n_estimators=i, random_state=10)\n",
    "    classifier = classifier.fit(train_X,train_y)\n",
    "    Y_pred = classifier.predict(valid_X)\n",
    "    acc=accuracy_score(valid_y, Y_pred)\n",
    "    if(acc>max_acc):\n",
    "        max_acc=acc\n",
    "        best_classifier=classifier\n",
    "        best_sts_num=i\n",
    "print(\"Adaboost classifier best validation accuracy is using \"+str(best_sts_num)+\" max depth with accuracy of \"+ str(max_acc))\n",
    "Y_pred = best_classifier.predict(test_X)\n",
    "print(\"testing accuracy with the best classifier is:\"+ str(accuracy_score(test_y, Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaa5a9b",
   "metadata": {},
   "source": [
    "we also got similar results in Adaboost - sklearn's model being slightly better than ours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca38e4fa",
   "metadata": {},
   "source": [
    "now for our regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8dc7b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, score=None, value=None,samples=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.score = score\n",
    "        self.value = value\n",
    "        self.samples=samples\n",
    "\n",
    "class DecisionTreeRegressor():\n",
    "    def __init__(self, min_samples_split=5, max_depth=2, X_idxs=[], Y_index=[], cols=[]):\n",
    "        self.root = None\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.X_idxs = X_idxs\n",
    "        self.Y_idx = Y_index\n",
    "        self.cols = cols\n",
    "    def build_tree(self, dataset, curr_depth=0):\n",
    "        num_samples = dataset.shape[0]\n",
    "        self.samples=num_samples\n",
    "        if num_samples >= self.min_samples_split and curr_depth < self.max_depth:\n",
    "            best_split = self.find_best_split(dataset)\n",
    "            if best_split[\"score\"] < self.SSR(dataset[self.cols[self.Y_idx]]):\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth + 1)\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth + 1)\n",
    "                return Node(best_split[\"feature\"], best_split[\"threshold\"],\n",
    "                            left_subtree, right_subtree, best_split[\"score\"])\n",
    "        leaf_value = self.leaf_value(dataset)\n",
    "        return Node(value=leaf_value,samples=len(dataset))\n",
    "\n",
    "    def find_best_split(self, dataset):\n",
    "        nump = dataset.to_numpy()\n",
    "        best_split = {}\n",
    "        best_split[\"score\"] = float(\"inf\")\n",
    "        for feature in self.X_idxs:\n",
    "            sorted_data = nump[nump[:, feature].argsort()]\n",
    "            ssr_g2 = self.SSR(dataset[self.cols[self.Y_idx]].values)\n",
    "            count_g2 = len(dataset)\n",
    "            mean_g2 = dataset[self.cols[self.Y_idx]].mean()\n",
    "            mean_g1, ssr_g1, count_g1, prev = 0, 0, 0, -1\n",
    "            for i in range(0, len(sorted_data)):\n",
    "                if prev != -1:\n",
    "                    threshold = (sorted_data[i][feature] + sorted_data[prev][feature]) / 2\n",
    "                else:\n",
    "                    threshold = sorted_data[i][feature]\n",
    "                score = ssr_g1 + ssr_g2\n",
    "                if prev != -1:\n",
    "                    if sorted_data[prev][feature] != sorted_data[i][feature]:\n",
    "                        if score < best_split[\"score\"]+3:#ignore the +3, it solves math accuarcy problem\n",
    "                            best_split[\"feature\"] = feature\n",
    "                            best_split[\"score\"] = score\n",
    "                            best_split[\"threshold\"] = threshold\n",
    "\n",
    "                if (prev == -1) & (score < best_split[\"score\"]):\n",
    "                    best_split[\"feature\"] = feature\n",
    "                    best_split[\"score\"] = score\n",
    "                    best_split[\"threshold\"] = threshold\n",
    "\n",
    "                count_g1, mean_g1, ssr_g1 = self.add_g1(count_g1, mean_g1, ssr_g1, sorted_data[i][self.Y_idx])\n",
    "                count_g2, mean_g2, ssr_g2 = self.remove_g2(count_g2, mean_g2, ssr_g2, sorted_data[i][self.Y_idx])\n",
    "                prev = i\n",
    "\n",
    "        best_split[\"dataset_left\"], best_split[\"dataset_right\"] = self.split(dataset, best_split[\"feature\"],\n",
    "                                                                             best_split[\"threshold\"])\n",
    "        return best_split\n",
    "\n",
    "    def SSR(self, column):\n",
    "        if (len(column) == 0):\n",
    "            return 0\n",
    "        avg = sum(column) / len(column)\n",
    "        ssr = 0\n",
    "        for c in column:\n",
    "            ssr += (c - avg) ** 2\n",
    "        return ssr\n",
    "\n",
    "    def remove_g2(self, count, mean, ssr, x):\n",
    "        count -= 1\n",
    "        ssr -= (x - mean) ** 2\n",
    "        old_me = mean\n",
    "        if(count==0):\n",
    "            return 0,0,0\n",
    "        mean = (mean * (count + 1) - x) / (count )\n",
    "        ssr -= (old_me - mean) ** 2 * count\n",
    "        return count, mean, ssr\n",
    "\n",
    "    def add_g1(self, count, mean, ssr, x):\n",
    "        count += 1\n",
    "        if (count == 0):\n",
    "            return 0, 0, 0\n",
    "        old_me = mean\n",
    "        mean = (mean * (count - 1) + x) / (count )\n",
    "        ssr += (old_me - mean) ** 2 * (count - 1)\n",
    "        ssr += (x - mean) ** 2\n",
    "        return count, mean, ssr\n",
    "\n",
    "    def split(self, dataset, feature, threshold):\n",
    "        dataset_right = dataset.loc[dataset[self.cols[feature]] >= threshold]\n",
    "        dataset_left = dataset.loc[dataset[self.cols[feature]] < threshold]\n",
    "        return dataset_left, dataset_right\n",
    "\n",
    "    def leaf_value(self, dataset):\n",
    "        return dataset[self.cols[self.Y_idx]].values.mean()\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        dataset = dataset.to_numpy()\n",
    "        length = len(dataset)\n",
    "        y = [self.predict_sample(index, self.root, dataset) for index in range(0, length)]\n",
    "        return y\n",
    "\n",
    "    def predict_sample(self, index, tree, dataset):\n",
    "        if tree.value != None: return tree.value\n",
    "        feature_val = dataset[index][tree.feature]\n",
    "        if feature_val < tree.threshold:\n",
    "            return self.predict_sample(index, tree.left, dataset)\n",
    "        else:\n",
    "            return self.predict_sample(index, tree.right, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d4569f",
   "metadata": {},
   "source": [
    "this is the class of the regression in the decision tree. first we start by defining a node in the tree \"Node\".\n",
    "then we defined the class of the decision tree regressor and we build it. then we start with the \"find_best_split\" function, that takes the dataset and transforms it into a numpy array, and then we defined an additional array \"threshold\" that saves the current split. this helped us lower the run time and improve the effeciency of our code.\n",
    "then we have methods that calculate the SSR and the split. at the end we have our predict functions.\n",
    "for our regression we used max_depth as a hyper parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3510c9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data.iloc[:8041, :]\n",
    "train_X=training_data.iloc[:,:-1]\n",
    "train_y=training_data.iloc[:,-1]\n",
    "validation_data = data.iloc[8041:10051, :]\n",
    "valid_X=validation_data.iloc[:,:-1]\n",
    "valid_y=validation_data.iloc[:,-1]\n",
    "testing_data = data.iloc[10051:, :]\n",
    "test_X=testing_data.iloc[:,:-1]\n",
    "test_y=testing_data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f84d4967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of 2 trees is 101291031923336.7\n",
      "MSE of 3 trees is 58287908667727.945\n",
      "MSE of 4 trees is 55269670801677.19\n",
      "MSE of 5 trees is 47700391589772.61\n",
      "MSE of 6 trees is 44769613565845.35\n",
      "MSE of 7 trees is 42703392660110.44\n",
      "MSE of 8 trees is 46852250704404.92\n",
      "MSE of 9 trees is 53978641156641.555\n",
      "best validation MSE is using 7 max depth with MSE of 42703392660110.44\n",
      "testing MSE with the best Regressor is:\n",
      "59137576315808.64\n",
      "Time it took for the tree to get built:20.14856195449829 seconds\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "X_indexes = [i for i in range(0, 7)]\n",
    "Y_index = 7\n",
    "\n",
    "start = time.time()\n",
    "best_mse = float(\"inf\")\n",
    "best_regressor = None\n",
    "\n",
    "for i in range(2, 10):\n",
    "    regressor = DecisionTreeRegressor(max_depth=i, X_idxs=X_indexes, Y_index=Y_index,cols=col_names)\n",
    "    regressor.root = regressor.build_tree(training_data)\n",
    "    Y_pred = regressor.predict(validation_data)\n",
    "    mse = mean_squared_error(valid_y, Y_pred)\n",
    "    print(\"MSE of \" + str(i) + \" trees is \" + str(mse))\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_regressor = regressor\n",
    "        best_sts_num = i\n",
    "print(\"best validation MSE is using \" + str(best_sts_num) +\" max depth with MSE of \" + str(best_mse))\n",
    "Y_pred = best_regressor.predict(testing_data)\n",
    "end = time.time()\n",
    "print(\"testing MSE with the best Regressor is:\")\n",
    "print(mean_squared_error(test_y, Y_pred))\n",
    "print(\"Time it took for the tree to get built:\"+str(end-start)+\" seconds\")\n",
    "print(\"****************************************************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227f14a1",
   "metadata": {},
   "source": [
    "Sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e35c7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101291031923336.7\n",
      "58287908667727.945\n",
      "55269670801677.19\n",
      "47892602488404.46\n",
      "49172922824029.09\n",
      "47175647313066.36\n",
      "45434652985715.12\n",
      "61049482757949.61\n",
      "Decision tree regressor best validation MSE is using 8 max depth with MSE of 45434652985715.12\n",
      "testing MSE with the best Regressor is:65250957700329.016\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_mse = float(\"inf\")\n",
    "best_regressor = None\n",
    "for i in range(2, 10):\n",
    "    regressor = tree.DecisionTreeRegressor(max_depth=i,random_state=10)\n",
    "    regressor.fit(train_X, train_y)\n",
    "    Y_pred = regressor.predict(valid_X)\n",
    "    mse = mean_squared_error(valid_y, Y_pred)\n",
    "    print(mse)\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_regressor = regressor\n",
    "        best_sts_num = i\n",
    "print(\"Decision tree regressor best validation MSE is using \" + str(best_sts_num) +\" max depth with MSE of \" + str(best_mse))\n",
    "Y_pred = best_regressor.predict(test_X)\n",
    "print(\"testing MSE with the best Regressor is:\"+ str(mean_squared_error(test_y, Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9487b9e",
   "metadata": {},
   "source": [
    "we can see that we got slightly better performance in our model than in sklearn's."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6ccbcd",
   "metadata": {},
   "source": [
    "our gradientBoost class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f464386",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoost:\n",
    "    def __init__(self, trees=[], trees_num=100, X_idxs=[], Y_idx=[], cols=[], learning_rate=0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.trees = trees\n",
    "        self.trees_num = trees_num\n",
    "        self.X_idxs = X_idxs\n",
    "        self.Y_idx = Y_idx\n",
    "        self.cols = cols\n",
    "\n",
    "    def buildGradBoost(self, dataset, new_pred=[]):\n",
    "        length = len(dataset)\n",
    "        if (len(new_pred) == 0):\n",
    "            new_pred = np.full(length, dataset[self.cols[self.Y_idx]].values.mean())\n",
    "        y = dataset[self.cols[self.Y_idx]].to_numpy()\n",
    "        while len(self.trees) < self.trees_num:\n",
    "            residuals = y - new_pred\n",
    "            dataset = dataset.iloc[:, :8]\n",
    "            dataset.insert(8, \"Residual\", residuals.tolist(), True)\n",
    "            self.cols.append(\"Residual\")\n",
    "            regressor = DecisionTreeRegressor(min_samples_split=3, max_depth=3, X_idxs=self.X_idxs, Y_index=8,\n",
    "                                              cols=self.cols)\n",
    "            regressor.root = regressor.build_tree(dataset)\n",
    "            self.trees.append(regressor)\n",
    "            preds = np.array(regressor.predict(dataset))\n",
    "            new_pred += preds * self.learning_rate\n",
    "        return new_pred\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        mean = dataset[self.cols[self.Y_idx]].values.mean()\n",
    "        nump = dataset.to_numpy()\n",
    "        fin_pred = np.full(len(dataset), mean)\n",
    "        for tree in self.trees:\n",
    "            y = np.array([tree.predict_sample(index, tree.root, nump) for index in range(0, len(dataset))])\n",
    "            fin_pred += self.learning_rate * y\n",
    "        return fin_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aef2b688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse accuracy = 72474113053781.94 with 10 trees\n",
      "mse accuracy = 48716032396703.43 with 20 trees\n",
      "mse accuracy = 42975727677916.24 with 30 trees\n",
      "mse accuracy = 41065960698005.49 with 40 trees\n",
      "mse accuracy = 40754297934529.66 with 50 trees\n",
      "mse accuracy = 40416925629373.23 with 60 trees\n",
      "mse accuracy = 40167993278796.59 with 70 trees\n",
      "mse accuracy = 39654654522216.48 with 80 trees\n",
      "mse accuracy = 39385426067793.516 with 90 trees\n",
      "mse accuracy = 39182041250956.55 with 100 trees\n",
      "mse accuracy = 38776751102046.016 with 110 trees\n",
      "mse accuracy = 38610569617295.25 with 120 trees\n",
      "mse accuracy = 39171946498333.07 with 130 trees\n",
      "mse accuracy = 39316331139571.945 with 140 trees\n",
      "mse accuracy = 39124280394759.36 with 150 trees\n",
      "GradBoost Regressor best validation MSE is using 120 trees with MSE of 38610569617295.25\n",
      "testing MSE\n",
      "38658262368599.2\n",
      "Time it took for the tree to get built:195.78203749656677 seconds\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "best_mse = float(\"inf\")\n",
    "trees = []\n",
    "best_regressor = None\n",
    "new_pred = []\n",
    "start = time.time()\n",
    "for i in range(10, 151, 10):\n",
    "    regressor = GradientBoost(trees=trees, trees_num=i, X_idxs=X_indexes, Y_idx=Y_index, cols=col_names)\n",
    "    new_pred = regressor.buildGradBoost(training_data, new_pred)\n",
    "    trees = regressor.trees\n",
    "    Y_pred = regressor.predict(validation_data)\n",
    "    mse = mean_squared_error(valid_y, Y_pred)\n",
    "    print(\"mse accuracy = \"+ str(mse)+ \" with \"+str(i)+ \" trees\")\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_regressor = regressor\n",
    "        best_sts_num = i\n",
    "Y_pred = best_regressor.predict(testing_data)\n",
    "print(\"GradBoost Regressor best validation MSE is using \"+str(best_sts_num)+\" trees with MSE of \"+ str(best_mse))\n",
    "end = time.time()\n",
    "print(\"testing MSE\")\n",
    "print(mean_squared_error(test_y, Y_pred))\n",
    "print(\"Time it took for the tree to get built:\"+str(end-start)+\" seconds\")\n",
    "print(\"****************************************************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1c2d31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72514475577369.55\n",
      "48684817969872.03\n",
      "43041820051615.73\n",
      "41092132130596.6\n",
      "40767816496028.82\n",
      "40365147154098.22\n",
      "40585432545532.96\n",
      "40078176820271.16\n",
      "39590052219781.12\n",
      "39001922201097.36\n",
      "39352090659379.87\n",
      "39689680509671.59\n",
      "40008057300331.195\n",
      "40782977851180.51\n",
      "40795568024874.42\n",
      "41233574315663.64\n",
      "41394662572102.8\n",
      "41449512115234.99\n",
      "41601056425290.234\n",
      "Gradient boost regressor best validation MSE is using 100 max depth with MSE of 39001922201097.36\n",
      "testing MSE with the best Regressor is:39446465635117.07\n"
     ]
    }
   ],
   "source": [
    "best_mse = float(\"inf\")\n",
    "best_regressor = None\n",
    "for i in range(10, 200,10):\n",
    "    regressor = GradientBoostingRegressor(random_state=0,n_estimators=i,max_depth=3)\n",
    "    regressor.fit(train_X, train_y)\n",
    "    Y_pred = regressor.predict(valid_X)\n",
    "    mse = mean_squared_error(valid_y, Y_pred)\n",
    "    print(mse)\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_regressor = regressor\n",
    "        best_sts_num = i\n",
    "print(\"Gradient boost regressor best validation MSE is using \" + str(best_sts_num) +\" max depth with MSE of \" + str(best_mse))\n",
    "Y_pred = best_regressor.predict(test_X)\n",
    "print(\"testing MSE with the best Regressor is:\"+ str(mean_squared_error(test_y, Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598084ba",
   "metadata": {},
   "source": [
    "our code ran slower than the sklearn's code, but we got slightly better accuracy than the sklearn code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2713333f",
   "metadata": {},
   "source": [
    "<b>Performance</b>\n",
    "We tried our best to cut down time complexity, we started of with really slow model and worked our way up,\n",
    "we used numpy arrays instead of dataframes which really helped the runtime go better, then we used the sort method for spliting which really helped as well, then we find a way to calculate SSR and gini with O(1) , NOTE : we didn't use any help for calculating SSR with O(1), we found our original way\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
